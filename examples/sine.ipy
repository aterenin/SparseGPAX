%load_ext autoreload
%autoreload 2
import jax
import jax.numpy as jnp
import jax.random as jr
import jax.scipy as jsp
import tensorflow_probability
tfp = tensorflow_probability.experimental.substrates.jax
tfk = tfp.math.psd_kernels
import haiku as hk
from jax.experimental import optix
import matplotlib.pyplot as plt
import sparsegpax

kernel = tfk.ExponentiatedQuadratic()
rng = hk.PRNGSequence(1)

initial_state = hk.transform(lambda: sparsegpax.SparseGaussianProcess(kernel, 1, 1, 11, 67, 17).get_initial_state())
model = hk.transform(lambda x,s: sparsegpax.SparseGaussianProcess(kernel, 1, 1, 11, 67, 17)(x,s))
prior = hk.transform(lambda x,s: sparsegpax.SparseGaussianProcess(kernel, 1, 1, 11, 67, 17).prior(x,s))
randomize = hk.transform(lambda s: sparsegpax.SparseGaussianProcess(kernel, 1, 1, 11, 67, 17).randomize(s))
resample_prior_basis = hk.transform(lambda s: sparsegpax.SparseGaussianProcess(kernel, 1, 1, 11, 67, 17).resample_prior_basis(s))
prior_kl = hk.transform(lambda s: sparsegpax.SparseGaussianProcess(kernel, 1, 1, 11, 67, 17).prior_KL(s))
err_stddev = hk.transform(lambda: sparsegpax.SparseGaussianProcess(kernel, 1, 1, 11, 67, 17).err_stddev())
hyperprior = hk.transform(lambda: sparsegpax.SparseGaussianProcess(kernel, 1, 1, 11, 67, 17).hyperprior())

@jax.jit
def loss(params,key,x,y,n_data,gpstate):
    gpstate = randomize.apply(params,key,gpstate)
    #
    kl = prior_kl.apply(params,jr.PRNGKey(0),gpstate)
    #
    f = model.apply(params,jr.PRNGKey(0),x,gpstate)
    s = err_stddev.apply(params,jr.PRNGKey(0))
    (n_samples,_,n_batch) = f.shape
    c = n_data / (n_batch * n_samples * 2)
    l = n_data*jnp.sum(jnp.log(s)) + c*jnp.sum(((y - f) / s)**2)
    #
    r = hyperprior.apply(params,jr.PRNGKey(0))
    #
    return kl + l + r

def plot(x,y,f):
    m = jnp.mean(f, axis=0)
    u = jnp.quantile(f, 0.975, axis=0)
    l = jnp.quantile(f, 0.025, axis=0)
    #
    (fig,ax) = plt.subplots()
    ax.scatter(x,y)
    ax.plot(x,m,linewidth=2)
    ax.fill_between(x, l, u, alpha=0.5)
    #
    for i in range(f.shape[0]):
        ax.plot(x,f[i,:], color="gray",alpha=0.5)
    #
    fig.show()

x = jnp.expand_dims(jnp.linspace(-5,5,101), -1)
y = 2 * jnp.sin(x).T + jr.normal(next(rng), x.T.shape)/10

params = initial_state.init(next(rng))
gpstate = initial_state.apply(params, next(rng))

params = hk.data_structures.merge(params,{
    'sparse_gaussian_process': {
        'inducing_locations': jnp.expand_dims(jnp.arange(-5,5.0000005,1), -1),
        }
    })
gpstate = resample_prior_basis.apply(params,next(rng),gpstate)
gpstate = randomize.apply(params,next(rng),gpstate)

loss(params,next(rng),x,y,x.shape[0],gpstate)

opt = optix.adam(0.01)
opt_state = opt.init(params)

for i in range(200):
    (train_loss,grads) = jax.value_and_grad(loss)(params,next(rng),x,y,x.shape[0],gpstate)
    (updates,opt_state) = opt.update(grads, opt_state)
    params = optix.apply_updates(params,updates)
    print(i,"Loss:",train_loss)
